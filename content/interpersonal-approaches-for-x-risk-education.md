---
permalink: interpersonal-approaches-for-x-risk-education
lw-was-draft-post: "false"
lw-is-af: "false"
lw-is-debate: "false"
lw-page-url: https://www.lesswrong.com/posts/pG7zuvMonHDCJFfjv/interpersonal-approaches-for-x-risk-education
lw-is-question: "true"
lw-posted-at: 2018-01-24T00:47:44.183000Z
lw-last-modification: None
lw-curation-date: None
lw-frontpage-date: None
lw-was-unlisted: "false"
lw-is-shortform: "false"
lw-num-comments-on-upload: 10
lw-base-score: 10
lw-vote-count: 8
af-base-score: 0
af-num-comments-on-upload: 0
publish: true
title: Interpersonal Approaches for X-Risk Education
lw-latest-edit: 2018-01-24T00:47:44.183000Z
lw-is-linkpost: "false"
tags:
  - community
aliases:
  - interpersonal-approaches-for-x-risk-education
lw-reward-post-warning: "false"
use-full-width-images: "false"
date_published: 2018-01-24 00:00:00
original_url: https://www.lesswrong.com/posts/pG7zuvMonHDCJFfjv/interpersonal-approaches-for-x-risk-education
skip_import: true
description: Effective ways to raise awareness of AI risk among researchers, weighing
  persuasion vs. education and status vs. receptiveness.
date_updated: 2024-11-22 20:04:30.137574
---




Much of the AI research community remains unaware of the Alignment Problem (according to my personal experience), and I haven't seen much discussion about how to deliberately expand the community (all I've seen to this effect is [Scott's A/B/C/D/E testing on alignment articles](http://slatestarcodex.com/2016/10/24/ai-persuasion-experiment-results/)).

Expanding the number of people aware of (and ideally, working on) the alignment problem is a high-leverage activity: a constant amount of effort spent educating someone in exchange for a chance of recruiting an ally who will work hard at our sides. Another metric by which we should evaluate approaches is whether we have to convince or simply educate; professors and high-status researchers may be more dismissive (possibly due to the [inside view](https://lesswrong.com/tag/outside_view), their wariness of strange-sounding ideas, and [overconfidence in their long-term predictions](https://www.newyorker.com/magazine/2005/12/05/everybodys-an-expert)), but their influence would be greater. On the other hand, a good friend in a CS or Math under-/post-graduate program may be more receptive.

In my case, I stumbled upon HP:MoR one year ago, read the Sequences, and then read more about Alignment and [CEV](https://intelligence.org/files/CEV.pdf). I appreciated that Alignment was a serious problem, but it wasn't until I got through _Superintelligence_ that I realized it's basically The Problem. Being in the second year of my doctorate program, I didn't know whether I was "too late" to start learning the math, "too far behind" people like Eliezer to make a difference. What I did know is that everyone can't defect - we need people to put in the work, and we probably need substantially more people doing so.

What happened to me took a lot of time and may be unrealistic to recommend to others. The articles Scott tested seem equally effective; instead, I'd like to discuss what social approaches work best for taking people from friend to friend-who-takes-alignment-seriously (while optimizing against effort expended), and whether this is an efficient use of our time.
