---
publish: "true"
hide_authors: "true"
hide_metadata: "true"
no_dropcap: "true"
permalink: posts
title: Posts & Sequences
aliases:
  - Posts
hideSubscriptionLinks: false
card_image: https://assets.turntrout.com/static/images/card_images/test_library.png
description: A selection of the best posts which I've written.
date_published: 2024-10-27 19:14:04.653922
date_updated: 2025-01-30 09:30:36.233182
tags:
  - website
---








# My favorite posts

1. [Looking back on my alignment PhD](/alignment-phd)
2. [The shard theory of human values](/shard-theory)
3. [Bruce Wayne and the cost of inaction](/bruce-wayne-and-the-cost-of-inaction)
4. [Formalizing “defection” using game theory](/what-counts-as-defection)
5. [You should read "Harry Potter and the Methods of Rationality"](/read-hpmor)

# My recent posts

![[recent#^recent-posts-listing]]

If you want, [try going a little further back](/recent) to see all of my posts.

# Explore by tag

![[all-tags#^all-tags-listing]]

# Sequences

Originally, my content [was hosted on LessWrong](https://www.lesswrong.com/users/turntrout). Much of that content was meant to be consumed as part of a series - or "sequence" - of blog posts.

## Becoming Stronger

In early 2018, I became convinced that [the AI alignment problem](https://en.wikipedia.org/wiki/AI_alignment) needed to be solved. Who, though, would solve it?

I didn't remember much formal math or computer science, but I wanted to give my all anyways. I started reading textbooks. I started reading _a lot_ of textbooks.

> [!quote] Original sequence description
>
> >[!quote] [The Verres family motto](https://hpmor.com/chapter/7)
> > You can never have enough books.
>
> My journey through the MIRI research guide.

![](https://assets.turntrout.com/static/images/posts/test_library.avif)

1. [Set Up for Success: Insights from "Naïve Set Theory"](/set-up-for-success-insights-from-naive-set-theory)
2. [Lightness and Unease](./lightness-and-unease.md)
3. [The Art of the Artificial: Insights from "Artificial Intelligence: A Modern Approach"](./the-art-of-the-artificial-insights-from-artificial.md)
4. [The First Rung: Insights from "Linear Algebra Done Right"](/the-first-rung-insights-from-linear-algebra-done-right.md)
5. [Internalizing Internal Double Crux](./internalizing-internal-double-crux.md)
6. [Confounded No Longer: Insights from "All of Statistics"](./confounded-no-longer-insights-from-all-of-statistics.md)
7. [Into the Kiln: Insights from Tao's "Analysis I"](/into-the-kiln-insights-from-tao-s-analysis-i.md)
8. [Swimming Upstream: A Case Study in Instrumental Rationality](./swimming-upstream)
9. [Making a Difference Tempore: Insights from "Reinforcement Learning: An Introduction"](./making-a-difference-tempore-insights-from-reinforcement)
10. [Turning Up the Heat: Insights from Tao's "Analysis II"](./second-analysis-textbook-review)
11. [And My Axiom! Insights from "Computability and Logic"](./computability-and-logic-textbook-review)
12. [Judgment Day: Insights from "Judgment in Managerial Decision Making"](./managerial-decision-making-review)
13. [Continuous Improvement: Insights from "Topology"](./topology-textbook-review)
14. [A Kernel of Truth: Insights from "A Friendly Approach to Functional Analysis"](./functional-analysis-textbook-review)
15. [Problem Relaxation as a Tactic](./problem-relaxation-as-a-tactic)
16. [Insights from Euclid's "Elements"](./insights-from-euclids-elements)
17. [Insights from "Modern Principles of Economics"](./insights-from-modern-principles-of-economics)
18. [Do a Cost-Benefit Analysis of Your Technology Usage](./digital-minimalism)
19. [Looking Back on my Alignment PhD](./alignment-phd)
20. [Insights from "The Manga Guide to Physiology"](./insights-from-physiology)

## Reframing Impact

> [!quote] Original sequence description
> Why do some things seem like really big deals to us? Do most agents best achieve their goals by seeking power? How might we avert catastrophic incentives in the utility maximization framework?

![](https://assets.turntrout.com/static/images/posts/reframing-impact-card.avif)

Introductory post: [Reframing Impact](./reframing-impact)

### Part 1: What is "impact"?

1. [Value Impact](./value-impact)
2. [Deducing Impact](./deducing-impact)
3. [Attainable Utility Theory: Why Things Matter](./attainable-utility-theory)
4. [World State is the Wrong Abstraction for Impact](./world-state-is-the-wrong-abstraction-for-impact)
5. [The Gears of Impact](./the-gears-of-impact)

### Part 2: How agents impact each other

1. [Seeking Power is Often Convergently Instrumental in MDPs](./seeking-power-is-often-convergently-instrumental-in-mdps)
2. [Attainable Utility Landscape: How The World Is Changed](./attainable-utility-landscape)
3. [The Catastrophic Convergence Conjecture](./the-catastrophic-convergence-conjecture)
4. [Attainable Utility Preservation: Concepts](./attainable-utility-preservation-concepts)

### Part 3: Regularizing impact

1. [Attainable Utility Preservation: Empirical Results](./attainable-utility-preservation-empirical-results)
2. [How Low Should Fruit Hang Before We Pick It?](./how-low-should-fruit-hang-before-we-pick-it)
3. [Attainable Utility Preservation: Scaling to Superhuman](./attainable-utility-preservation-scaling-to-superhuman)
4. [Reasons for Excitement about Impact of Impact Measure Research](./excitement-about-impact-measures)
5. [Conclusion to "Reframing Impact"](./conclusion-to-reframing-impact)

## The Causes of Power-Seeking and Instrumental Convergence

This sequence generalizes the math of [Seeking Power is Often Convergently Instrumental in MDPs](./seeking-power-is-often-convergently-instrumental-in-mdps). The posts follow up on [Seeking Power is Often Convergently Instrumental in MDPs](./seeking-power-is-often-convergently-instrumental-in-mdps) and [The Catastrophic Convergence Conjecture](./the-catastrophic-convergence-conjecture).

<!-- vale off -->
> [!quote] Original sequence description
> Instrumental convergence posits that smart goal-directed agents will tend to take certain actions (e.g. gain resources, stay alive) in order to achieve their goals. These actions seem to involve taking power _from_ humans. Human disempowerment seems like a key part of how AI might go very, very wrong.
>
> But where does instrumental convergence come from? When does it occur, and how strongly? And what does the math look like?
<!-- vale on -->

![](https://assets.turntrout.com/static/images/posts/power-seeking-AI.avif)

> [!warning] [Reward is not the optimization target](./reward-is-not-the-optimization-target)
> Many posts in this sequence treat reward functions as "specifying goals", in some sense. This is [wrong](./reward-is-not-the-optimization-target), as I have [argued at length](./against-inner-outer-alignment). Reward signals are akin to a per-datapoint learning rate. Reward chisels circuits into the AI. That's it!

1. [Power as Easily Exploitable Opportunities](./power-as-easily-exploitable-opportunities)
2. [Generalizing POWER to Multi-Agent Games](./formalizing-multi-agent-power)
3. [MDP Models Are Determined by the Agent Architecture and the Environment](./MDPs-are-not-subjective)
4. [Environmental Structure Can Cause Instrumental Convergence](./environmental-structure-can-cause-instrumental-convergence)
5. [A World in Which the Alignment Problem Seems Lower-Stakes](./lower-stakes-alignment-scenario)
6. [The More Power at Stake, the Stronger Instrumental Convergence](./quantitative-strength-of-instrumental-convergence)
7. [Seeking Power Is Convergently Instrumental in a Broad Class of Environments](./power-seeking-beyond-MDPs)
8. [When Most VNM-Coherent Preference Orderings Have Convergent Instrumental Incentives](./instrumental-convergence-via-vnm-preferences)
9. [Satisficers Tend to Seek Power: Instrumental Convergence via Retargetability](./satisficers-tend-to-seek-power)
10. [Instrumental Convergence for Realistic Agent Objectives](./instrumental-convergence-for-realistic-agent-objectives)
11. [Parametrically Retargetable Decision-Makers Tend to Seek Power](./parametrically-retargetable-power-seeking)

## Thoughts on Corrigibility

<figure class="float-right desktop-only" style="margin-top:-1rem; width: 80%;">
<img src="https://assets.turntrout.com/static/images/posts/hal_9000.avif" alt="" loading="lazy" style="width: 80%;">
</figure>

> [!quote] Original sequence description
>
> My writings on different kinds of corrigibility. These thoughts build on each other and form part of my alignment worldview (circa 2021), but they are not yet woven into a coherent narrative.

1. [Non-Obstruction: A Simple Concept Motivating Corrigibility](./non-obstruction-motivates-corrigibility)
2. [Corrigibility As Outside View](./corrigibility-as-outside-view)
3. [A Certain Formalization of Corrigibility is VNM-Incoherent](./a-certain-formalization-of-corrigibility-is-vnm-incoherent)
4. [Formalizing Policy Modification Corrigibility](./formalizing-policy-modification-corrigibility)

## Shard Theory

In early 2022, [Quintin Pope](https://www.linkedin.com/in/quintin-pope/) and I noticed glaring problems at the heart of "classical" alignment arguments. We thought through the problem with fresh eyes and derived _shard theory_.

Classical arguments focus on what _the_ goal of an AI will be. Why? There's not a good answer that I've ever heard. Shard theory redirects our attention from fixed single objectives. The basic upshot of shard theory: AIs and humans are well-understood[^experimental] as having a bunch of situationally activated goals -- "shards" of desire and preference.

For example, you probably care more about people you can see. Shard theory predicts this outcome. Consider your learned decision-making circuits which bid for actions which care for your friend Bill. These circuits were probably formed when you were able to see Bill (or perhaps the vast majority of your "caring about people" circuits were formed when physically around people). If you can see Bill, that situation is more "similar to the training distribution" for your "caring about Bill" shard. Therefore, the Bill shard is especially likely to fire when you can see him.

[^experimental]: Team Shard's [later experimental work](./understanding-and-controlling-a-maze-solving-policy-network) provided strong evidence that AI policies are not just _well-understood_ as having shards, but in fact mechanistically learn a blend of multiple situationally activated goals.

Thus, [it seems OK if our AIs don't have "perfect" shard mixtures](./alignment-without-total-robustness). The stronger their "aligned shards", the more human welfare weighs on their decision-making. We're playing a game of inches, so let's play to win.

![](https://assets.turntrout.com/static/images/posts/human_shards.avif)

1. [Humans Provide an Untapped Wealth of Evidence About Alignment](./humans-provide-alignment-evidence)
2. [Human Values & Biases Are Inaccessible to the Genome](./human-values-and-biases-are-inaccessible-to-the-genome)
3. [General Alignment Properties](./general-alignment-properties)
4. [Evolution Is a Bad Analogy for AGI: Inner Alignment](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/FyChg3kYG54tEN3u6)
5. [Reward Is Not the Optimization Target](./reward-is-not-the-optimization-target)
6. [The Shard Theory of Human Values](./shard-theory)
7. [Understanding and Avoiding Value Drift](./understanding-and-avoiding-value-drift)
8. [A Shot at the Diamond-Alignment Problem](./a-shot-at-the-diamond-alignment-problem)
9. [Don't Design Agents Which Exploit Adversarial Inputs](./dont-design-agents-which-exploit-adversarial-inputs)
10. [Don't Align Agents to Evaluations of Plans](./dont-align-agents-to-evaluations-of-plans)
11. [Alignment Allows "Nonrobust" Decision-Influences and Doesn't Require Robust Grading](./alignment-without-total-robustness)
12. [Inner and Outer Alignment Decompose One Hard Problem Into Two Extremely Hard Problems](./against-inner-outer-alignment)

## Interpreting a Maze-Solving Network

My work with my MATS 3.0 scholars, [Ulisse Mini](https://uli.rocks) and [Peli Grietzer](https://thegradientpub.substack.com/p/peli-grietzer-a-mathematized-philosophy)!

![](https://assets.turntrout.com/static/images/posts/shard_mouse.avif)
Figure: A [mouse with cheese subshards](https://imgur.com/a/doRBRs4).

> [!quote] Original sequence description
>
> Mechanistic interpretability on a pretrained policy network from [Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/abs/2105.14111).

1. [Predictions for Shard Theory Mechanistic Interpretability Results](./predictions-for-shard-theory-mechanistic-interpretability)
2. [Understanding and Controlling a Maze-Solving Policy Network](./understanding-and-controlling-a-maze-solving-policy-network)
3. [Maze-Solving Agents: Add a Top-Right Vector, Make the Agent Go to the Top-Right](./top-right-steering-vector)
4. [Behavioral Statistics for a Maze-Solving Agent](./statistics-of-a-maze-solving-network)
