---
permalink: the-gears-of-impact
lw-was-draft-post: 'false'
lw-is-af: 'true'
lw-is-debate: 'false'
lw-page-url: https://www.lesswrong.com/posts/coQCEe962sjbcCqB9/the-gears-of-impact
lw-is-question: 'false'
lw-posted-at: 2019-10-07T14:44:51.212000Z
lw-last-modification: None
lw-curation-date: None
lw-frontpage-date: 2019-10-07T06:10:45.213000Z
lw-was-unlisted: 'false'
lw-is-shortform: 'false'
lw-num-comments-on-upload: 16
lw-base-score: 54
lw-vote-count: 18
af-base-score: 18
af-num-comments-on-upload: 0
publish: true
title: The Gears of Impact
lw-latest-edit: 2024-08-15T22:30:43.246000Z
lw-is-linkpost: 'false'
tags:
  - understanding-the-world
  - impact-regularization
aliases:
  - the-gears-of-impact
lw-sequence-title: Reframing Impact
lw-sequence-image-grid: sequencesgrid/izfzehxanx48hvf10lnl
lw-sequence-image-banner: sequences/zpia9omq0zfhpeyshvev
sequence-link: posts#reframing-impact
prev-post-slug: world-state-is-the-wrong-abstraction-for-impact
prev-post-title: World State is the Wrong Abstraction for Impact
next-post-slug: seeking-power-is-often-convergently-instrumental-in-mdps
next-post-title: Seeking Power is Often Convergently Instrumental in MDPs
lw-reward-post-warning: 'false'
use-full-width-images: 'false'
date_published: 2019-10-07 00:00:00
original_url: https://www.lesswrong.com/posts/coQCEe962sjbcCqB9/the-gears-of-impact
skip_import: true
card_image: https://assets.turntrout.com/static/images/card_images/PFqi66W.png
description: 'Impact reframed: a gears-level view of how and why some things seem
  important to us.'
date_updated: 2025-03-22 12:22:59.421452
---







![](https://assets.turntrout.com/static/images/posts/hKhkvwg.avif)

![](https://assets.turntrout.com/static/images/posts/IXogCtA.avif)

![](https://assets.turntrout.com/static/images/posts/2r2DVFx.avif)

![](https://assets.turntrout.com/static/images/posts/holekcV.avif)

![](https://assets.turntrout.com/static/images/posts/SzFSiEc.avif)

![](https://assets.turntrout.com/static/images/posts/wCRzqox.avif)

![](https://assets.turntrout.com/static/images/posts/BAWF2q1.avif)

![](https://assets.turntrout.com/static/images/posts/UCGx4QR.avif )

![](https://assets.turntrout.com/static/images/posts/5YOlvLh.avif)

![](https://assets.turntrout.com/static/images/posts/yA8wkQP.avif)

![](https://assets.turntrout.com/static/images/posts/QXG2pVK.avif)

![](https://assets.turntrout.com/static/images/posts/27F0KkU.avif)

![](https://assets.turntrout.com/static/images/posts/B7rMciV.avif)

![](https://assets.turntrout.com/static/images/posts/HIfRI7r.avif)

![](https://assets.turntrout.com/static/images/posts/ye9suf7.avif)

![](https://assets.turntrout.com/static/images/posts/sMgB7yR.avif)

![](https://assets.turntrout.com/static/images/posts/lQ1jYfB.avif )

![](https://assets.turntrout.com/static/images/posts/b6pDiKi.avif)

![](https://assets.turntrout.com/static/images/posts/iRLXEeH.avif)

![](https://assets.turntrout.com/static/images/posts/uRr6YqY.avif )

![](https://assets.turntrout.com/static/images/posts/67uR5SE.avif)

![](https://assets.turntrout.com/static/images/posts/PFqi66W.avif)

![](https://assets.turntrout.com/static/images/posts/GBVahyL.avif)

![](https://assets.turntrout.com/static/images/posts/SATKmJJ.avif)

![](https://assets.turntrout.com/static/images/posts/v338kDc.avif)

![](https://assets.turntrout.com/static/images/posts/oqEeta9.avif)

![](https://assets.turntrout.com/static/images/posts/epI7152.avif)

![](https://assets.turntrout.com/static/images/posts/dvVEmBs.avif)

â€‹![](https://assets.turntrout.com/static/images/posts/HShpS3u.avif)

![](https://assets.turntrout.com/static/images/posts/WjTqF2y.avif)

![](https://assets.turntrout.com/static/images/posts/dLUrki7.avif)

![](https://assets.turntrout.com/static/images/posts/lDbQW2b.avif )

> [!exercise]
> Why does instrumental convergence happen? Would it be coherent to imagine a reality without it?

# Notes

- Here, our descriptive theory relies on our ability to have reasonable beliefs about what we'll do, and how things in the world will affect our later decision-making process. No one knows how to formalize that kind of reasoning, so I'm leaving it a black box: we _somehow_ have these reasonable beliefs which are _apparently_ used to calculate AU.
- In technical terms, AU calculated with the "could" criterion would be closer to an optimal value function, while actual AU seems to be an on-policy prediction, _whatever that means_ in the embedded context. Felt impact corresponds to TD error.
- Framed as a kind of EU, we plausibly use AU to make decisions.
- I'm not claiming normatively that "embedded agentic" EU _should_ be AU; I'm simply using "embedded agentic" as an adjective.
