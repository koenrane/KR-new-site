---
permalink: value-impact
lw-was-draft-post: 'false'
lw-is-af: 'true'
lw-is-debate: 'false'
lw-page-url: https://www.lesswrong.com/posts/TxcYSRQ9giC6zmKov/value-impact
lw-is-question: 'false'
lw-posted-at: 2019-09-23T00:47:12.991000Z
lw-last-modification: None
lw-curation-date: None
lw-frontpage-date: 2019-09-23T05:20:26.521000Z
lw-was-unlisted: 'false'
lw-is-shortform: 'false'
lw-num-comments-on-upload: 10
lw-base-score: 70
lw-vote-count: 28
af-base-score: 22
af-num-comments-on-upload: 3
publish: true
title: Value Impact
lw-latest-edit: 2019-09-23T00:47:13.121000Z
lw-is-linkpost: 'false'
tags:
  - understanding-the-world
  - impact-regularization
aliases:
  - value-impact
lw-sequence-title: Reframing Impact
lw-sequence-image-grid: sequencesgrid/izfzehxanx48hvf10lnl
lw-sequence-image-banner: sequences/zpia9omq0zfhpeyshvev
sequence-link: posts#reframing-impact
prev-post-slug: reframing-impact
prev-post-title: Reframing Impact
next-post-slug: deducing-impact
next-post-title: Deducing Impact
lw-reward-post-warning: 'false'
use-full-width-images: 'false'
date_published: 2019-09-23 00:00:00
original_url: https://www.lesswrong.com/posts/TxcYSRQ9giC6zmKov/value-impact
skip_import: true
card_image: https://assets.turntrout.com/static/images/card_images/D6Bhmv6.png
description: Impact is objectively important to agents, no matter their goals. Even
  robots hoarding pebbles care about a meteor strike.
date_updated: 2025-03-22 12:22:59.421452
---






![](https://assets.turntrout.com/static/images/posts/lG9je1g.avif)

![](https://assets.turntrout.com/static/images/posts/1hJa51n.avif)

![](https://assets.turntrout.com/static/images/posts/geDXLLG.avif)

![](https://assets.turntrout.com/static/images/posts/zMxBlb0.avif)

![](https://assets.turntrout.com/static/images/posts/tupgltr.avif)![](https://assets.turntrout.com/static/images/posts/kNG5for.avif)

![](https://assets.turntrout.com/static/images/posts/BtzHnUq.avif)

![](https://assets.turntrout.com/static/images/posts/jaHW2pp.avif)

![](https://assets.turntrout.com/static/images/posts/MmtIR5e.avif)

![](https://assets.turntrout.com/static/images/posts/S1KiiUj.avif)

![](https://assets.turntrout.com/static/images/posts/9ZqUDO6.avif)

![](https://assets.turntrout.com/static/images/posts/TT61fRC.avif)

![](https://assets.turntrout.com/static/images/posts/JGA0KAj.avif)

![](https://assets.turntrout.com/static/images/posts/ZBG9SXA.avif)

![](https://assets.turntrout.com/static/images/posts/IhjRIpN.avif)

![](https://assets.turntrout.com/static/images/posts/lsJLMDk.avif)

![](https://assets.turntrout.com/static/images/posts/OBmQUKm.avif)

> ! Being on Earth when this happens is a big deal, no matter your objectives – you can't hoard pebbles if you're dead! People would feel the loss from anywhere in the cosmos. However, Pebblehoarders wouldn't mind if they weren't in harm's way.

![](https://assets.turntrout.com/static/images/posts/zKM6Bt9.avif)

![](https://assets.turntrout.com/static/images/posts/ZOAeuoe.avif)

![](https://assets.turntrout.com/static/images/posts/fzLD7kQ.avif)

## Appendix: Contrived Objectives

A natural definitional objection is that a few agents aren't affected by objectively impactful events. If you think every outcome is equally good, then who cares if the meteor hits?

Obviously, our values aren't like this, and any agent we encounter or build is unlikely to be like this (since these agents wouldn't do much). Furthermore, these agents seem contrived in a technical sense (low measure under reasonable distributions in a reasonable formalization), as we'll see later. That is, "most" agents aren't like this.

From now on, assume we aren't talking about this kind of agent.

# Notes

- Eliezer [introduced Pebblesorters in the Sequences](https://www.readthesequences.com/Sorting-Pebbles-Into-Correct-Heaps); I made them robots here to better highlight how pointless the pebble transformation is to humans.
- In informal parts of the sequence, I'll often use "values", "goals", and "objectives" interchangeably, depending on what flows.
- We're going to lean quite a bit on thought experiments and otherwise speculate on mental processes. While I've taken the obvious step of beta-testing the sequence and randomly peppering my friends with strange questions to check their intuitions, maybe some of the conclusions only hold for people like me. I mean, [some people don't have mental imagery](https://www.lesswrong.com/posts/baTWMegR42PAsH9qJ/generalizing-from-one-example)– who would've guessed? Even if my intuitive answers don't generalize, the goal is to find an impact measure. Deducing human universals would just be a bonus.
- Objective impact is objective with respect to the agent's _values_ – it is _not_ the case that an objective impact affects you anywhere and anywhen in the universe! If someone finds $100, that matters for agents at that point in space and time (no matter their goals), but it doesn't mean that everyone in the universe is objectively impacted by one person finding some cash!
- If you think about it, the phenomenon of objective impact is _surprising._ See, in AI alignment, we're used to no-free-lunch this, no-universal-argument that. The possibility of something objectively important to agents hints that our perspective has been incomplete. It hints that maybe this "impact" thing underlies a key facet of what it means to interact with the world. It hints that even if we saw specific instances of this before, we didn't know we were looking at, and we didn't stop to ask.
