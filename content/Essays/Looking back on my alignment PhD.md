---
permalink: alignment-phd
publish: "true"
original_url: https://www.lesswrong.com/posts/2GxhAyn9aHqukap2S/looking-back-on-my-alignment-phd
date_published: 06/30/2022
---
This post has been recorded as part of the LessWrong Curated Podcast. It can be listened to on [Spotify](https://open.spotify.com/episode/5UY1LrzUakTbs8LsL0ld1s?si=adfa86cec2c4409a), [Apple Podcasts](https://podcasts.apple.com/us/podcast/looking-back-on-my-alignment-phd-by-turntrout/id1630783021?i=1000569310113), and [Libsyn](https://sites.libsyn.com/421877/looking-back-on-my-alignment-phd).

---

<figure>
<img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/6ddc0291a1961469101cbd7d8516c7ffa43d6b6711dc7c36.png" alt="" style="max-width: 550px; width: 100%; margin-left:¬†auto;  
¬† margin-right:¬†auto; display: block">
 <figcaption>My <a href="https://arxiv.org/pdf/2206.11831.pdf" class="external-link">dissertation.</a> It‚Äôs long, so if you‚Äôre going to read anything from it, read Chapter 0 (Introduction).</figcaption></figure>

The funny thing about long periods of time is that they do, eventually, come to an end. I‚Äôm proud of what I accomplished during my PhD. That said, I‚Äôm going to first focus on mistakes I‚Äôve made over the past four[^1] years. 

# Mistakes

I think I [got significantly smarter in 2018‚Äì2019](https://www.lesswrong.com/s/KGYLvTqFiFE2CpHfJ/p/eAsN5vNjvmxzACuuX), and kept learning some in 2020‚Äì2021. I _was_ significantly less of a fool in 2021 than I was in 2017. That is important and worth feeling good about. But all things considered, I still made a lot of profound mistakes over the course of my PhD.¬†

## Social dynamics distracted me from my mission

> [!failure] I focused on ‚Äúcatching up‚Äù to other thinkers
> I wanted to be more like Eliezer Yudkowsky and Buck Shlegeris and Paul Christiano. They know lots of facts and laws about lots of areas (e.g. general relativity and thermodynamics and information theory). I focused on building up dependencies (like [analysis](https://www.lesswrong.com/s/KGYLvTqFiFE2CpHfJ/p/cuZxipMFup5uJdeAp) and [geometry](https://www.lesswrong.com/s/KGYLvTqFiFE2CpHfJ/p/Wpf3Gsa8A89mmjkk8) and [topology](https://www.lesswrong.com/s/KGYLvTqFiFE2CpHfJ/p/TSLnckszv4Tb5cHmt)) not only because I wanted to know the answers, but because I felt I owed a _debt_, that I was _in the red_ until I could at least meet other thinkers at their level of knowledge.¬†
> 
> But rationality is not about the bag of facts you know, nor is it about the concepts you have internalized. Rationality is about _how_ your mind holds itself, it is _how_ you weigh evidence, it is _how_ you decide where to look next when puzzling out a new area.¬†
> 
> If I had been more honest with myself, I could have nipped the ‚Äúcatching up with other thinkers‚Äù mistake in 2018. I could have removed the bad mental habits using [certain introspective techniques](https://www.lesswrong.com/tag/internal-double-crux); or at least been aware of the badness.
> 
> But I did not, in part because the truth was uncomfortable. If I did not have a clear set of prerequisites (e.g. analysis and topology and game theory) to work on, I would not have a clear and immediate direction of improvement. I would have felt adrift.
> 
> But there is not yet any ‚Äúrationality tech tree‚Äù, no succession of well-defined rationality skills such that you can learn them in order and grow way stronger. Like, you can‚Äôt just do the [calibration exercises](http://acritch.com/credence-game/), and then [the noticing-confusion exercises](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=nL7E6SEtXqDG7SHGB), and then other things. Those tools help, but they aren‚Äôt _enough_. There won‚Äôt be a clear and immediate direction of improvement, at first. But you may want to get stronger anyways.
> 
> _I figured this point out by summer 2021._¬†

> [!failure] I focused on seeming smart and defensible
> 
> When I started working on alignment, I didn‚Äôt know what to do at first, and I felt insecure about my credentials. As far as I remember, I figured I‚Äôd start off by becoming respected, since other people‚Äôs feedback was initially a better guide than my own taste. Unfortunately, I didn‚Äôt realize how deeply and subtly this goal would grow its roots. ¬†
> 
> I worried about upvotes, I worried about winning arguments, I worried about being defensible against criticism. I was so worried that someone would comment on one of my posts and tear everything down, because I _hadn‚Äôt been careful enough_, because I had _left myself open_ by not dotting all my ‚Äòi‚Äôs. (Not that anyone has ever done that on LessWrong before‚Ä¶)
> 
> I think it was this year that I had my (second) ‚Äúoh man, _don‚Äôt forget the part where everyone is allowed to die to AI_‚Äù moment. To illustrate the new mindset this gut-realization gave me, I‚Äôll detail a recent decision with social consequences, and then compare the old and the new mindsets.¬†
> 
> A few months back, Quintin Pope approached me with (what he claimed to be) a new alignment paradigm, which blossomed from asking the following kind of questions:
> 
> > We clearly prefer future AIs to generalize in the way that neuroscientists generalize, so it seems worthwhile to ask: ‚Äúwhy don‚Äôt neuroscientists wirehead themselves?‚Äù
> >
> > It‚Äôs clearly not because humans evolved away from wireheading,¬†_specifically_. There are somewhat similar situations to wireheading in the ancestral environment: psychoactive drugs, masturbation, etc. Is the reason we don‚Äôt wirehead because evolution instilled us with an aversion to manipulating our reward function, which then zero-shot generalized to wireheading, despite wireheading being so wildly dissimilar to the contents of the ancestral environment? How could evolution have developed an alignment approach that generalized so well?
> 
> After a few days, I realized my gut expectations were that he was broadly correct and that this theory of alignment could actually be right. However, I realized I wasn‚Äôt consciously letting myself think that because it would be Insufficiently Skeptical to actually think the alignment problem is solvable. This seemed obviously stupid to me, so I quickly shut that line of thinking down and second-order updated towards optimism so that I would [stop _predictably_ getting more optimistic](https://www.readthesequences.com/Conservation-Of-Expected-Evidence) about Quintin‚Äôs theory.[^2]
> 
> I realized I assigned about 5% credence to ‚Äúthis line of thinking marks a direct and reasonably short path to solving alignment.‚Äù Thus, on any calculation of benefits and harms, I should be willing to stake some reputation to quickly get more eyeballs on the theory, even though I expected to end up looking a little silly (with about 95% probability). With my new attitude, I decided ‚Äúwhatever, let‚Äôs just get on with it and stop wasting time.‚Äù¬†
> 
> The old ‚Äúdon‚Äôt leave any avenue of being criticized!‚Äù attitude would have been less loyal to my true beliefs. I would have told myself: ‚ÄúThis _could_ work, but there are so many parts I don‚Äôt understand yet. If I figure those parts out first, I can explain it better and avoid having to go out on a limb in the process.‚Äù Cowardice and social anxiety, dressed up as prudence and skepticism.
>  
> I still get anxious around disagreements with people I respect. I am still working on fully expunging the ‚Äúdefensibility‚Äù urges, because they suck. But I‚Äôve already made a lot of progress.[^3]
>  
> _I figured this point out_ [_this spring_](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=hhuLBManRziZXbkHo#f3Mzhn8Gmf8XGyz9y).

> [!failure] Too much deference, too little thinking for myself 
> 
> I filtered the world through a status lens. If I read a comment from a high-status person, I would gloss over confusing parts, because _I_ was probably the one reading it wrong. Sure, I would verbally agree that [modest epistemology](https://equilibriabook.com/inadequacy-and-modesty/) is unproductive. I just _happened_ to not think thoughts like ‚ÄúHIGH-STATUS PERSON‚Äôs claim seems obviously dumb and wrong.‚Äù
>  
> Now I let myself think thoughts like that, and it‚Äôs great. For example, last week I was reading about Pavlov‚Äôs conditioning experiments with dogs. I read the following:
>  
> > Pavlov (1902) started from the idea that there are some things that a dog does not need to learn. For example, dogs don‚Äôt learn to salivate whenever they see food. This reflex is ‚Äòhard-wired‚Äô into the dog.
>  
> I thought, ‚Äúthat seems like bullshit. Really, the dogs are _hard-wired_ to salivate when they _see_ food? Doesn‚Äôt that require _hard-wiring a food-classifier into the dog‚Äôs brain_?!‚Äù¬†
> 
> And you know what? It _was_ bullshit. I searched for about 8 minutes before finding references of [the original lectures Pavlov gave](https://psychclassics.yorku.ca/Pavlov/lecture2.htm):
> 
> > Dr. Zitovich took several young puppies away from their mother and fed them for considerable time only on milk. When the puppies were a few months old he established fistulae of their salivary ducts, and was thus able to measure accurately the secretory activity of the glands. _He now showed these puppies some solid food -- bread or meat -- but no secretion of saliva was evoked._ 
> 
> Our world is so inadequate that seminal psychology experiments are described in mangled, misleading ways. Inadequacy abounds, and status only weakly tracks adequacy. Even if the high-status person belongs to your in-group. Even if all your smart friends are nodding along.
> 
> Would you notice if _this very post_ were inadequate and misleading? _Would_ it be bullshit for the dog-genome to hardwire a food-classifier? _Think for yourself. Constant vigilance!_
> 
> _I realized and started fixing this mistake [this spring.](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=hhuLBManRziZXbkHo#f3Mzhn8Gmf8XGyz9y)_
## Non-social mistakes
> [!failure] I thought about comfortable, familiar problems
> 
> I remember a sunny summer day in 2019, sitting in the grass with Daniel Filan at UC Berkeley. He recommended putting together an end-to-end picture of the alignment problem. I remember feeling pretty uncomfortable about that, feeling that I wouldn‚Äôt understand which alignment problems go where in my diagram (‚Äúdo embedded agency failures crop up _here_, or _there_?‚Äù). Wouldn‚Äôt it just make more sense to read more alignment papers and naturally refine those views over time?
> 
> This was a rationalization, plain and simple. There is no point where you feel ready to put all the pieces together. If you feel totally comfortable about how alignment fits together such that Daniel‚Äôs exercise does not _push you_ on some level, we have either _already_ solved the alignment problem, or you are deluded.¬†
> 
> I did not feel ready, and I was not ready, and I should have done it anyways. But I focused on more comfortable work with well-defined boundaries, because it felt good to knock out new theorems. Whether or not those theorems were useful and important to alignment, that was a mistake. So I stayed in my alignment comfort zone. I should have stopped working on impact measures and power-seeking way earlier than I did, even though I did end up doing some cool work.
> 
> _I figured this point out this spring, because I bumped into Quintin as described above._¬†

> [!failure] Not admitting to myself that I thought alignment was doomed
> 
> After I became more optimistic about alignment due to having a sharper understanding of the overall problem and of how human values formed to begin with, I also became more pessimistic about _other_ approaches, like IDA, ELK, RRM, AUP, or anything else with a three-letter acronym. But my new understanding didn‚Äôt seem to present any _specific_ objections. So why did I suddenly feel worse about these older ideas?
> 
> I _suspect_ that part of the explanation is: I hadn‚Äôt wanted to admit how confused I was about alignment, and I (implicitly) clutched to ‚Äúbut it _could_ work‚Äù-style hopefulness. But now that I had a _different_ reason to hope, resting upon a more solid and mechanistic understanding, _now_ it was apparently emotionally safe for me to admit I didn‚Äôt have much hope at all for the older approaches.¬†
> 
> Yikes.¬†
> 
> If that‚Äôs what happened, I was seriously deluding myself. I will do better next time.
> 
> _Figured this out this spring. I‚Äôm not sure if I‚Äôve fixed the general error yet._

> [!failure]  I viewed my life through narratives
> 
> Back in 2018, I had the ‚Äúupstart alignment researcher‚Äù narrative‚Äîstarting off bright-eyed and earnest, learning a lot, making friends. But then I hurt my hands and couldn‚Äôt type anymore, which broke the narrative. I felt dejected‚Äîto slightly exaggerate, I felt I had fallen off of the sunlit path, and now nothing was going to go as it should.¬†
> 
> Another example of narrative-thinking is when people say ‚ÄúI‚Äôm just not a math person.‚Äù This is an _inference_ and a _story_ they tell themselves. Strictly speaking, they may not know much math, and they may not enjoy math, and they may not see how to change either of those facts. But the _narrative_ is that they are not a math person. Their discomfort and their aversion-to-trying stem not just from their best-guess assessment of their own weaknesses, but from a _story_ they are living in.¬†
> 
> Every moment is an opportunity for newly-directed action. [Keep your identity small](http://www.paulgraham.com/identity.html) and keep the narratives in the story-books. At least, if you want to use narratives, carefully introspect to make sure you‚Äôre using them, and they aren‚Äôt using you.¬†
> 
> _I probably figured this point out in 2021._
# Two helpful habits I picked up¬†

I‚Äôm not really sure where these two habits go, so I‚Äôll put them here. I wish I‚Äôd had these skills in 2018.¬†

> [!tip] Distinguish between _**observations**_ and _**inferences**_
> 
> When people speak to you, mark their arguments as _observations_ or as _inferences_. Keep the types _separate_. I‚Äôve gained _so much_ from this simple practice.¬†
> 
> Here are two cases I‚Äôve recently found where people seem to mistake the folk wisdom for observation: 
> 
> 1. ‚ÄúPeople often say they‚Äôre afraid to die‚Äù is an _observation,_ and ‚Äúpeople are hard-wired to be afraid of death‚Äù is an _inference._ 
> 2. ‚ÄúI often feel ‚Äòcuriosity‚Äô and some kind of exploration-impulse‚Äù is an _observation,_ and ‚Äúpeople are innately curious‚Äù is an _inference._

> [!tip] Be [concrete](https://www.lesswrong.com/posts/XosKB3mkvmXMZ3fBQ/specificity-your-brain-s-superpower) 
> 
> My friend Kurt remarks that I constantly ask for examples. If a friend comes to me for advice and says ‚ÄúI‚Äôm terrible at dating, I just feel so shy!‚Äù, I _could_ say ‚ÄúYou‚Äôre really fun to be around, you‚Äôre probably just in your head too much‚Äù, and then _they_ could say ‚ÄúAgh, maybe, but it‚Äôs just so frustrating.‚Äù Wouldn‚Äôt that just be such a useful conversation for them? That‚Äôll _definitely_ solve their awkwardness!
> 
> Alternatively, if I _ask for an example_, we can both analyze an event which _actually happened_. Perhaps they say, ‚ÄúI met a girl named Alice at the party, but I somehow ran out of things to say, and it got quiet, and we found excuses to part ways.‚Äù Then I can help my friend introspect and figure out why they didn‚Äôt have anything to say, which _is in fact a question with a real answer_.
> 
> The general rhythm is: Bind your thinking to _coherent scenarios_ (preferably ones which _actually happened_, like meeting a girl named Alice), so that you (and possibly other people) can explore the details together (like why it got quiet) in order to figure out what to change (like running mock encounters to shoo away the social anxiety).¬†
> 
> On the other hand, if you can‚Äôt think of a concrete example to ground your airy words, maybe your thinking is totally untethered from reality. Maybe your assumptions are _contradictory_ and you can‚Äôt even see it.¬†
> 
> Here‚Äôs something I recently said on Discord:
> 
> > If there are some circuits who can defer to the market prediction, then each circuit can get their coalitional contribution as their fixed weight. This lets some relatively simpler circuits retain weight. At least, those are the abstract words I want to say, but now I feel confused about how to apply that to a concrete example for how e.g. a shallow but broad ‚Äúdon‚Äôt steal‚Äù value negotiates via [Critch-bargaining](https://arxiv.org/abs/1711.00363). _Not being able to give a concrete example means I don‚Äôt really know what I‚Äôm talking about here._
>  
> As another example, don‚Äôt tell me how your alignment strategy will e.g. ‚Äúfaithfully reproduce human judgments.‚Äù Explain [what concrete benefits you hope to realize](https://www.lesswrong.com/posts/d4YGxMpzmvxknHfbe/conversation-with-eliezer-what-do-you-want-the-system-to-do), and why ‚Äúfaithful reproduction of human judgments‚Äù will realize those benefits. ¬†
> 
> If the actual answer is that you _don‚Äôt know_, then just _say it_, because it‚Äôs the truth. Be aware that you don‚Äôt know.

Overall, I wish I‚Äôd expected more from myself. I wish I‚Äôd believed myself capable of building an end-to-end picture of the alignment problem, of admitting what I didn‚Äôt know and what I hadn‚Äôt thought about, of being able to survive/ignore the harsh winds of criticism and skepticism.¬†

I did these things eventually, though, and I‚Äôm proud of that.

# What I‚Äôm proud of

> [!success] I [didn't keep working on computational chemistry](https://www.lesswrong.com/posts/eAsN5vNjvmxzACuuX/swimming-upstream-a-case-study-in-instrumental-rationality) 
> 
> Boy howdy, would that have been awful for me. _Thank you,_ TurnTrout<sub>2018</sub>!¬†
> 
> I remember thinking ‚ÄúYou know what, I‚Äôd rather get _expelled_ than not do the 2018 [CHAI](https://humancompatible.ai/) internship.‚Äù This thought gave me the courage to find a new advisor who would let me work on AI safety, funding be damned. (I‚Äôm not a natural nonconformist. Conflict makes me nervous. I‚Äôve had to work for it.¬†)

> [!success] I [learned a lot of math](https://www.lesswrong.com/s/KGYLvTqFiFE2CpHfJ), even though I felt sheepish and insecure about it at first

> [!success] I think I ended up achieving rationality escape velocity
>1. When I get stuck / feel depressed, errors get thrown, exception-handling activates, I start thinking ‚Äúthese thoughts seem unreasonably dark; my cognition is compromised; have I eaten enough food today, have I drank enough water, should I call a friend‚Ä¶‚Äù.¬†
>2. When I get stuck on a problem (e.g. what is the type signature of human values?), I do not stay stuck. I notice I am stuck, I run down a list of tactics, I explicitly note what works, I upweight that for next time.¬†
>3. When I realize I‚Äôve been an idiot about something (e.g. nicking my hand with a knife, missing a deadline), I stop and think _wow, that was stupid, what‚Äôs the more general error I‚Äôm making_?
> 
>The general rhythm is: I feel agentic and capable and self-improving, and these traits are strengthening over time, as is the rate of strengthening. This definitely didn‚Äôt have to happen, but I made it happen (with the help of some friends and resources).

> [!success] I‚Äôm proud of my research achievements
>
> 1. I think that [_Reframing Impact_](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW) correctly inferred our intuitions around what ‚Äúimpact‚Äù means, and also that sequence was beautiful and I loved making it.¬†
> 2. [My dissertation](https://arxiv.org/pdf/2206.11831.pdf) is also beautiful. I painstakingly wrote and formatted and edited it, even hiring a professional to help out. I fought to keep its tone focused on what matters: the sharp dangers of AGI.¬†
> 3. I likewise poured myself into [_Optimal Policies Tend To Seek Power_](https://arxiv.org/abs/1912.01683) and its follow-up, [_Parametrically Retargetable Decision-Makers Tend To Seek Power_](https://arxiv.org/abs/2206.13477).
> 	  1. First, I had felt instrumental convergence should be provable and formally understandable. It was a mystery to me in 2019, and now it‚Äôs not.¬†
> 	  2. Second, I used to suck at writing academic papers, but I managed to get two NeurIPS spotlights by the end of my program. NeurIPS spotlights might not save the world, but that was tough and I did a good job with it.¬†
> 1. [Attainable utility preservation](https://arxiv.org/abs/1902.09725) is pointless for AGI alignment, but _damn_ is it cool that we could do unsupervised learning to get a reward function, preserve the agent‚Äôs ability to optimize that single random objective, and [_just get cautious behavior in complicated environments_](https://arxiv.org/abs/2006.06547).

# Looking forward

Leaving Oregon was a bit sad, but coming to Berkeley is exciting. I‚Äôll be starting my CHAI postdoc soon. I‚Äôm working with lots of cool, smart, loyal friends. I‚Äôm feeling strong and confident and relatively optimistic, both about alignment and about my personal future.¬†

[Here's to winning](https://www.lesswrong.com/posts/Cf2zBkoocqcjnrNFD/emotionally-confronting-a-probably-doomed-world-against). ü•Ç

[^1]: My PhD was six years long (it started in the fall of 2016). However, I‚Äôm not even going to critique the first two years, because that would make the ‚ÄúMistakes‚Äù section far too long.
[^2]: If you‚Äôre interested in reading about the theory now, see [this recent comment](https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer?commentId=EfeMSnBvbvxjSQBc3). I‚Äôm currently putting together some prerequisite posts to bridge the inferential gap. _Later published: The [shard theory of human values.](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values)_
[^3]: Sometimes I feel the urge to defend myself _just a little more_, to which some part of me internally replies ‚Äúare you serious, this defensibility thing again?! Are you _ever_ going to let me _actually think_?‚Äù I like that part of me a lot.
