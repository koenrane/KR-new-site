---
permalink: alignment-phd
lw-was-draft-post: "false"
lw-is-af: "false"
lw-is-debate: "false"
lw-page-url: https://www.lesswrong.com/posts/2GxhAyn9aHqukap2S/looking-back-on-my-alignment-phd
lw-is-question: "false"
lw-posted-at: 2022-07-01T03:19:59.497000Z
lw-last-modification: 2022-07-20T02:22:00.482000Z
lw-curation-date: 2022-07-09T22:39:04.885000Z
lw-frontpage-date: 2022-07-01T03:26:02.441000Z
lw-was-unlisted: "false"
lw-is-shortform: "false"
lw-num-comments-on-upload: 64
lw-base-score: 331
lw-vote-count: 149
af-base-score: 101
af-num-comments-on-upload: 0
publish: true
title: Looking back on my alignment PhD
lw-latest-edit: 2023-08-10T01:35:59.613000Z
lw-is-linkpost: "false"
tags:
  - growth-stories
  - AI
  - rationality
  - personal
aliases:
  - looking-back-on-my-alignment-phd
  - phd
  - PhD
  - PHD
lw-podcast-link: https://www.buzzsprout.com//2037297/11168051-looking-back-on-my-alignment-phd-by-turntrout.js?container_id=buzzsprout-player-11168051&amp;player=small
lw-sequence-title: Becoming Stronger
lw-sequence-image-grid: sequencesgrid/fkqj34glr5rquxm6z9sr
lw-sequence-image-banner: sequences/oerqovz6gvmcpq8jbabg
sequence-link: posts#becoming-stronger
prev-post-slug: digital-minimalism
prev-post-title: Do a cost-benefit analysis of your technology usage
lw-reward-post-warning: "false"
use-full-width-images: "false"
original_url: https://www.lesswrong.com/posts/2GxhAyn9aHqukap2S/looking-back-on-my-alignment-phd
date_published: 2022-06-30 00:00:00
skip_import: true
card_image: https://assets.turntrout.com/static/images/card_images/6ddc0291a1961469101cbd7d8516c7ffa43d6b6711dc7c36.png
description: The mistakes made, the lessons learned, and the drive to solve a hilariously
  neglected super-problem.
date_updated: 2025-03-01 17:42:48.379662
next-post-slug: insights-from-physiology
next-post-title: Insights from "The Manga Guide to Physiology"
---










> [!info]
>
> This post has been recorded as part of the LessWrong Curated Podcast. It can be listened to on [Spotify](https://open.spotify.com/episode/5UY1LrzUakTbs8LsL0ld1s?si=adfa86cec2c4409a), [Apple Podcasts](https://podcasts.apple.com/us/podcast/looking-back-on-my-alignment-phd-by-turntrout/id1630783021?i=1000569310113), and [Libsyn](https://sites.libsyn.com/421877/looking-back-on-my-alignment-phd).

<figure>
<img src="https://assets.turntrout.com/static/images/posts/6ddc0291a1961469101cbd7d8516c7ffa43d6b6711dc7c36.avif" alt="" style="max-width: 550px; width: 100%; margin-left:Â auto;  
Â  margin-right:Â auto; display: block">
 <figcaption>My <a href="https://arxiv.org/pdf/2206.11831.pdf" class="external-link">dissertation.</a> It's long, so if you're going to read anything from it, read Chapter 0 (Introduction).</figcaption></figure>

The funny thing about long periods of time is that they do, eventually, come to an end. I'm proud of what I accomplished during my PhD. That said, I'm going to first focus on mistakes I've made over the past four[^1] years.

![](https://assets.turntrout.com/static/images/posts/phdReflection.avif)
Figure: Before my thesis defense -- thinking about what I'd done to advance alignment research, and if it would be enough.

# Mistakes

I think I [got significantly smarter in 2018â€“2019](/swimming-upstream), and kept learning some in 2020â€“2021. I _was_ significantly less of a fool in 2021 than I was in 2017. That is important and worth feeling good about. But all things considered, I still made a lot of profound mistakes over the course of my PhD.

## Social dynamics distracted me from my core mission

> [!failure] I focused on "catching up" to other thinkers
>
> Subtitle: I figured this point out by summer 2021.
>
> I wanted to be more like Eliezer Yudkowsky and Buck Shlegeris and Paul Christiano. They know lots of facts and laws about lots of areas (e.g. general relativity and thermodynamics and information theory). I focused on building up dependencies (like [analysis](/first-analysis-textbook-review) and [geometry](/insights-from-euclids-elements) and [topology](/topology-textbook-review)) not only because I wanted to know the answers, but because I felt I owed a _debt_, that I was _in the red_ until I could at least meet other thinkers at their level of knowledge.
>
> Rationality is not about the bag of facts you know, nor is it about the concepts you have internalized. Rationality is about _how_ your mind holds itself, it is _how_ you weigh evidence, it is _how_ you decide where to look next when puzzling out a new area.
>
> If I had been more honest with myself, I could have nipped the "catching up with other thinkers" mistake in 2018. I could have removed the bad mental habits using [certain introspective techniques](/internalizing-internal-double-crux), or at least have been aware of the badness.
>
> I did not, in part because the truth was uncomfortable. If I did not have a clear set of prerequisites (e.g. analysis and topology and game theory) to work on, I would not have a clear and immediate direction of improvement. I would have felt adrift.
>
> You cannot yet follow any "rationality tech tree", no succession of well-defined rationality skills such that you can learn them in order and grow way stronger. Like, you can't just do the [calibration exercises](http://acritch.com/credence-game/), and then [the noticing-confusion exercises](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=nL7E6SEtXqDG7SHGB), and then other things. Those tools help, but they aren't _enough_. There won't be a clear and immediate direction of improvement at first. But you may want to get stronger anyways.

> [!failure] I focused on seeming smart and defensible
>
> Subtitle: I figured this point out [this spring](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=hhuLBManRziZXbkHo#f3Mzhn8Gmf8XGyz9y).
>
> When I started working on alignment, I didn't know what to do at first, and I felt insecure about my credentials. As far as I remember, I figured I'd start off by becoming respected, since other people's feedback was initially a better guide than my own taste. Unfortunately, I didn't realize how deeply and subtly this goal would grow its roots.
>
> I worried about upvotes, I worried about winning arguments, I worried about being defensible against criticism. I was so worried that someone would comment on one of my posts and tear everything down, because I _hadn't been careful enough_, because I had _left myself open_ by not dotting all my 'i's. (Not that anyone has ever done that on LessWrong beforeâ€¦)
>
> I think it was this year that I had my (second) "oh man, _don't forget the part where everyone is allowed to die to AI_" moment. To illustrate the new mindset this gut-realization gave me, I'll detail a recent decision with social consequences, and then compare the old and the new mindsets.
>
> A few months back, Quintin Pope approached me with (what he claimed to be) a new alignment paradigm, which blossomed from asking the following kind of questions:
>
> > We clearly prefer future AIs to generalize in the way that neuroscientists generalize, so it seems worthwhile to ask: "why don't neuroscientists wirehead themselves?"
> >
> > It's clearly not because humans evolved away from wireheading,Â _specifically_. There are somewhat similar situations to wireheading in the ancestral environment: psychoactive drugs, masturbation, etc. Is the reason we don't wirehead because evolution instilled us with an aversion to manipulating our reward function, which then zero-shot generalized to wireheading, despite wireheading being so wildly dissimilar to the contents of the ancestral environment? How could evolution have developed an alignment approach that generalized so well?
>
> After a few days, I realized my gut expectations were that he was broadly correct and that this theory of alignment could actually be right. However, I realized I wasn't consciously letting myself think that because it would be Insufficiently Skeptical to actually think the alignment problem is solvable. This seemed obviously stupid to me, so I quickly shut that line of thinking down and second-order updated towards optimism so that I would [stop _predictably_ getting more optimistic](https://www.readthesequences.com/Conservation-Of-Expected-Evidence) about Quintin's theory.
>
> I realized I assigned about 5% credence to "this line of thinking marks a direct and reasonably short path to solving alignment." Thus, on any calculation of benefits and harms, I should be willing to stake some reputation to quickly get more eyeballs on the theory, even though I expected to end up looking a little silly (with about 95% probability). With my new attitude, I decided "whatever, let's just get on with it and stop wasting time."
>
> The old "don't leave any avenue of being criticized!" attitude would have been less loyal to my true beliefs. I would have told myself: "This _could_ work, but there are so many parts I don't understand yet. If I figure those parts out first, I can explain it better and avoid risky public declarations." Cowardice and social anxiety, dressed up as prudence and skepticism.
>
> I still get anxious around disagreements with people I respect. I am still working on fully expunging the "defensibility" urges, because they suck. But I've already made a lot of progress.[^3]

> [!failure] Too much deference, too little thinking for myself
>
> Subtitle: I realized and started fixing this mistake [this spring.](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=hhuLBManRziZXbkHo#f3Mzhn8Gmf8XGyz9y)
>
> I filtered the world through a status lens. If I read a comment from a high-status person, I would gloss over confusing parts, because _I_ was probably the one reading it wrong. Sure, I would verbally agree that [modest epistemology](https://equilibriabook.com/inadequacy-and-modesty/) is unproductive. I just _happened_ to not think thoughts like "HIGH-STATUS PERSON's claim seems obviously dumb and wrong."
>
> Now I let myself think thoughts like that, and it's great. For example, last week I was reading about Pavlov's conditioning experiments with dogs. I read the following:
>
> > Pavlov (1902) started from the idea that there are some things that a dog does not need to learn. For example, dogs don't learn to salivate whenever they see food. This reflex is 'hard-wired' into the dog.
>
> I thought, "that seems like bullshit. Really, the dogs are _hard-wired_ to salivate when they _see_ food? Doesn't that require _hard-wiring a food-classifier into the dog's brain_?!"
>
> And you know what? It _was_ bullshit. I searched for about 8 minutes before finding references of [the original lectures Pavlov gave](https://psychclassics.yorku.ca/Pavlov/lecture2.htm):
>
> > Dr. Zitovich took several young puppies away from their mother and fed them for considerable time only on milk. When the puppies were a few months old he established fistulae of their salivary ducts, and was thus able to measure accurately the secretory activity of the glands. _He now showed these puppies some solid food -- bread or meat -- but no secretion of saliva was evoked._
>
> Our world is so inadequate that seminal psychology experiments are described in mangled, misleading ways. Inadequacy abounds, and status only weakly tracks adequacy. Even if the high-status person belongs to your in-group. Even if all your smart friends are nodding along.
>
> Would you notice if _this post_ were inadequate and misleading? _Would_ it be bullshit for the dog-genome to hardwire a food-classifier? _Think for yourself. Constant vigilance!_

### Non-social mistakes

> [!failure] I thought about comfortable, familiar problems
>
> Subtitle: I figured this point out this spring because I bumped into Quintin (as described above).
>
> I remember a sunny summer day in 2019, sitting in the grass with Daniel Filan at UC Berkeley. He recommended putting together an end-to-end picture of the alignment problem. I remember feeling pretty uncomfortable about that, feeling that I wouldn't understand which alignment problems go where in my diagram ("do embedded agency failures crop up _here_, or _there_?"). Wouldn't it just make more sense to read more alignment papers and naturally refine those views over time?
>
> This was a rationalization, plain and simple. There is no point where you feel ready to put all the pieces together. If you feel totally comfortable about how alignment fits together such that Daniel's exercise does not _push you_ on some level, we have either _already_ solved the alignment problem, or you are deluded.
>
> I did not feel ready, and I was not ready, and I should have done it anyways. But I focused on more comfortable work with well-defined boundaries, because it felt good to knock out new theorems. Whether or not those theorems were useful and important to alignment, that was a mistake. So I stayed in my alignment comfort zone. I should have stopped working on impact measures and power-seeking way earlier than I did, even though I did end up doing some cool work.

> [!failure] Not admitting to myself that I thought alignment was doomed
>
> Subtitle: Figured this out this spring. I'm not sure if I've fixed the general error yet.
>
> After I became more optimistic about alignment due to having a sharper understanding of the overall problem and of how human values formed to begin with, I also became more pessimistic about _other_ approaches, like IDA, ELK, RRM, AUP, or anything else with a three-letter acronym. But my new understanding didn't seem to present any _specific_ objections. So why did I suddenly feel worse about these older ideas?
>
> I _suspect_ that part of the explanation is: I hadn't wanted to admit how confused I was about alignment, and I (implicitly) clutched to "but it _could_ work"-style hopefulness. But now that I had a _different_ reason to hope, resting upon a more solid and mechanistic understanding, _now_ it was apparently emotionally safe for me to admit I didn't have much hope at all for the older approaches.
>
> Yikes.
>
> If that's what happened, I was seriously deluding myself. I will do better next time.

> [!failure] I viewed my life through narratives
>
> Subtitle: I probably figured this point out in 2021.
>
> Back in 2018, I had the "upstart alignment researcher" narrativeâ€”starting off bright-eyed and earnest, learning a lot, making friends. But then I hurt my hands and couldn't type anymore, which broke the narrative. I felt dejectedâ€”to slightly exaggerate, I felt I had fallen off of the sunlit path, and now nothing was going to go as it should.
>
> Another example of narrative-thinking is when people say "I'm just not a math person." This is an _inference_ and a _story_ they tell themselves. Strictly speaking, they may not know much math, and they may not enjoy math, and they may not see how to change either of those facts. But the _narrative_ is that they are not a math person. Their aversion stems not just from their best-guess assessment of their own weaknesses, but from a _story_ they are living in.
>
> Every moment is an opportunity for newly directed action. [Keep your identity small](http://www.paulgraham.com/identity.html) and keep the narratives in the story-books. At least, if you want to use narratives, carefully introspect to make sure you're using them, and they aren't using you.

## Two helpful habits I picked up

I'm not really sure where these two habits go, so I'll put them here. I wish I'd had these skills in 2018.

> [!tip] Distinguish between _**observations**_ and _**inferences**_
>
> When people speak to you, mark their arguments as _observations_ or as _inferences_. Keep the types _separate_. I've gained _so much_ from this simple practice.
>
> Here are two cases I've recently found where people seem to mistake the folk wisdom for observation:
>
> 1. "People often say they're afraid to die" is an _observation,_ and "people are hard-wired to be afraid of death" is an _inference._
> 2. "I often feel 'curiosity' and some kind of exploration-impulse" is an _observation,_ and "people are innately curious" is an _inference._

> [!tip] Be [concrete](https://www.lesswrong.com/posts/XosKB3mkvmXMZ3fBQ/specificity-your-brain-s-superpower)
>
> My friend Kurt remarks that I constantly ask for examples. If a friend comes to me for advice and says "I'm terrible at dating, I just feel so shy!", I _could_ say "You're really fun to be around, you're probably just in your head too much", and then _they_ could say "Agh, maybe, but it's just so frustrating." Wouldn't that just be such a useful conversation for them? That'll _definitely_ solve their awkwardness!
>
> Alternatively, if I _ask for an example_, we can both analyze an event which _actually happened_. Perhaps they say, "I met a girl named Alice at the party, but I somehow ran out of things to say, and it got quiet, and we found excuses to part ways." Then I can help my friend introspect and figure out why they didn't have anything to say, which _is in fact a question with a real answer_.
>
> The general rhythm is: Bind your thinking to _coherent scenarios_ (preferably ones which _actually happened_, like meeting a girl named Alice), so that you (and possibly other people) can explore the details together (like why it got quiet) in order to figure out what to change (like running mock encounters to shoo away the social anxiety).
>
> On the other hand, if you can't think of a concrete example to ground your airy words, maybe your thinking is totally untethered from reality. Maybe your assumptions are _contradictory_ and you can't even see it.
>
> Here's something I recently said on Discord:
>
> > If there are some circuits who can defer to the market prediction, then each circuit can get their coalitional contribution as their fixed weight. This lets some relatively simpler circuits retain weight. At least, those are the abstract words I want to say, but now I feel confused about how to apply that to a concrete example for how e.g. a shallow but broad "don't steal" value negotiates via [Critch-bargaining](https://arxiv.org/abs/1711.00363). _Not being able to give a concrete example means I don't really know what I'm talking about here._
>
> As another example, don't tell me how your alignment strategy will e.g. "faithfully reproduce human judgments." Explain [what concrete benefits you hope to realize](https://www.lesswrong.com/posts/d4YGxMpzmvxknHfbe/conversation-with-eliezer-what-do-you-want-the-system-to-do), and why "faithful reproduction of human judgments" will realize those benefits.
>
> If the actual answer is that you _don't know_, then just _say it_, because it's the truth. Be aware that you don't know.

To close out the "Mistakes" section, I mostly wish I'd expected more from myself. I wish I'd believed myself capable of building an end-to-end picture of the alignment problem, of admitting what I didn't know and what I hadn't thought about, of being able to survive/ignore the harsh winds of criticism and skepticism.

I did these things eventually, though, and I'm proud of that.

# What I'm proud of

> [!success] I [didn't keep working on computational chemistry](/swimming-upstream)
>
> Boy howdy, would that have been awful for me. _Thank you,_ `TurnTrout`<sub>2018</sub>!
>
> I remember thinking "You know what, I'd rather get _expelled_ than not do the 2018 [CHAI](https://humancompatible.ai/) internship." This thought gave me the courage to find a new advisor who would let me work on AI safety, funding be damned. (I'm not a natural nonconformist. Conflict makes me nervous. I've had to work for it.Â )

> [!success] I [learned a lot of math](/posts#becoming-stronger), even though I felt sheepish and insecure about it at first

> [!success] I think I ended up achieving rationality escape velocity
>
> 1. When I get stuck / feel depressed, errors get thrown, exception-handling activates, I start thinking "these thoughts seem unreasonably dark; my cognition is compromised; have I eaten enough food today, have I drank enough water, should I call a friendâ€¦".
> 2. When I get stuck on a problem (e.g. what is the type signature of human values?), I do not stay stuck. I notice I am stuck, I run down a list of tactics, I explicitly note what works, I upweight that for next time.
> 3. When I realize I've been an idiot about something (e.g. nicking my hand with a knife, missing a deadline), I stop and think _wow, that was stupid, what's the more general error I'm making_?
>
> The general rhythm is: I feel agentic and capable and self-improving, and these traits are strengthening over time, as is the rate of strengthening. This definitely didn't have to happen, but I made it happen (with the help of some friends and resources).

> [!success] I'm proud of my research achievements
>
> 1. I think that [_Reframing Impact_](/posts#reframing-impact) correctly inferred our intuitions around what "impact" means, and also that sequence was beautiful and I loved making it.
> 2. [My dissertation](https://arxiv.org/pdf/2206.11831.pdf) is also beautiful. I painstakingly wrote and formatted and edited it, even hiring a professional to help out. I fought to keep its tone focused on what matters: the sharp dangers of AGI.
> 3. I likewise poured myself into [_Optimal Policies Tend To Seek Power_](https://arxiv.org/abs/1912.01683) and its follow-up, [_Parametrically Retargetable Decision-Makers Tend To Seek Power_](https://arxiv.org/abs/2206.13477).
>    1. First, I had felt instrumental convergence should be provable and formally understandable. It was a mystery to me in 2019, and now it's not.
>    2. Second, I used to suck at writing academic papers, but I managed to get two NeurIPS spotlights by the end of my program. NeurIPS spotlights might not save the world, but that was tough and I did a good job with it.
> 4. [Attainable utility preservation](https://arxiv.org/abs/1902.09725) is pointless for AGI alignment, but _damn_ is it cool that we could do unsupervised learning to get a reward function, preserve the agent's ability to optimize that single random objective, and [_just get cautious behavior in complicated environments_](https://arxiv.org/abs/2006.06547).

## Looking forward

Leaving Oregon was a bit sad, but coming to Berkeley is exciting. I'll be starting my CHAI postdoc soon. I'm working with lots of cool, smart, loyal friends. I'm feeling strong and confident and relatively optimistic, both about alignment and about my personal future.

[Here's to winning](/emotionally-confronting-doom). ðŸ¥‚

[^1]: My PhD was six years long (it started in the fall of 2016). However, I'm not even going to critique the first two years, because that would make the "Mistakes" section far too long.
[^3]: Sometimes I feel the urge to defend myself _just a little more_, to which some part of me internally replies "are you serious, this defensibility thing again?! Are you _ever_ going to let me _actually think_?". I like that part of me a lot.
